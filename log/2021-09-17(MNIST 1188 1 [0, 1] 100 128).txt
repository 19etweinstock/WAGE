2021-09-17 14:15:22 

import time
import tensorflow as tf

tf.compat.v1.disable_eager_execution()
tf.compat.v1.disable_v2_behavior()

bitsW = 1  # bit width of weights
bitsA = 1  # bit width of activations
bitsG = 8  # bit width of gradients
bitsE = 8 # bit width of errors

bitsR = 1  # bit width of randomizer

lr_schedule = [0,1]

Epoch = 100

batchSize = 128

dataSet = 'MNIST'  # 'MNIST','SVHN','CIFAR10', 'ILSVRC2012'

def upper(str):
    return str.upper()

Time = time.strftime('%Y-%m-%d', time.localtime())
Notes = f'{dataSet} {bitsW}{bitsA}{upper(hex(bitsG)[2:])}{upper(hex(bitsE)[2:])} {bitsR} {lr_schedule} {Epoch} {batchSize}'
# Notes = 'lenet5 2888'
# Notes = 'alexnet 28CC'

GPU = [0]
validNum = 0


loadModel = None
# loadModel = '../model/' + '2017-10-26' + '(' + 'vgg7 2888' + ')' + '.tf'
# saveModel = None
saveModel = '../model/' + Time + '(' + Notes + ')' + '.tf'


use_batch_norm = False
lr = tf.compat.v1.Variable(initial_value=0., trainable=False, name='lr', dtype=tf.float32)
# lr_schedule = [0, 8, 200, 1,250,1./8,300,0]
# lr_schedule = [0, 32, 40, 32./8, 60, 32./64, 80, 0]
L2 = 0

lossFunc = 'SSE'
# lossFunc = tf.losses.softmax_cross_entropy
optimizer = tf.compat.v1.train.GradientDescentOptimizer(1)  # lr is controlled in Quantize.G

# shared variables, defined by other files
seed = None
sess = None
W_scale = []

def upper(str):
    return str.upper()

W: /device:GPU:0 Conv/conv0/ [5, 5, 1, 32] Scale:4
W: /device:GPU:0 Conv/conv1/ [5, 5, 32, 64] Scale:32
W: /device:GPU:0 Fc/fc0/ [1024, 512] Scale:32
W: /device:GPU:0 Fc/fc1/ [512, 10] Scale:16
CONV: 52000 FC: 529408 Total: 581408
W: /device:GPU:0 Conv_1/conv0/ [5, 5, 1, 32] Scale:4
W: /device:GPU:0 Conv_1/conv1/ [5, 5, 32, 64] Scale:32
W: /device:GPU:0 Fc_1/fc0/ [1024, 512] Scale:32
W: /device:GPU:0 Fc_1/fc1/ [512, 10] Scale:16

Optimization Start!

lr: 0.000000 -> 1.000000
Epoch: 000  Loss: 670.919872 Train: 0.4769 Test: 0.5062 FPS: 7592 
Epoch: 001  Loss: 649.798077 Train: 0.4934 Test: 0.4999 FPS: 11693 S BEST 
Epoch: 002  Loss: 642.177350 Train: 0.4886 Test: 0.5559 FPS: 10960 
Epoch: 003  Loss: 647.525641 Train: 0.5180 Test: 0.5979 FPS: 11037 
Epoch: 004  Loss: nan Train: 0.9347 Test: 1.0000 FPS: 10659 
Epoch: 005  Loss: nan Train: 1.0000 Test: 1.0000 FPS: 10824 
Epoch: 006  Loss: nan Train: 1.0000 Test: 1.0000 FPS: 10204 
Epoch: 007  Loss: nan Train: 1.0000 Test: 1.0000 FPS: 10712 
Epoch: 008  Loss: nan Train: 1.0000 Test: 1.0000 FPS: 11010 
Epoch: 009  Loss: nan Train: 1.0000 Test: 1.0000 FPS: 10842 
Epoch: 010  Loss: nan Train: 1.0000 Test: 1.0000 FPS: 11008 
Epoch: 011  Loss: nan Train: 1.0000 Test: 1.0000 FPS: 10876 
Epoch: 012  Loss: nan Train: 1.0000 Test: 1.0000 FPS: 10944 
Epoch: 013  Loss: nan Train: 1.0000 Test: 1.0000 FPS: 10540 
Epoch: 014  Loss: nan Train: 1.0000 Test: 1.0000 FPS: 11104 
Epoch: 015  Loss: nan Train: 1.0000 Test: 1.0000 FPS: 10537 
Epoch: 016  Loss: nan Train: 1.0000 Test: 1.0000 FPS: 10530 
Epoch: 017  Loss: nan Train: 1.0000 Test: 1.0000 FPS: 10725 
Epoch: 018  Loss: nan Train: 1.0000 Test: 1.0000 FPS: 10320 
Epoch: 019  Loss: nan Train: 1.0000 Test: 1.0000 FPS: 10641 
Epoch: 020  Loss: nan Train: 1.0000 Test: 1.0000 FPS: 10829 
Epoch: 021  